<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Minwoo Cho</title>

    <meta name="author" content="Minwoo Cho">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Minwoo Cho
                </p>
                <p>I am an M.S. student in Computer Science at KAIST, specializing in Reinforcement Learning (RL) and Robotics. Under the supervision of Prof. <a href="https://sites.google.com/site/daehyungpark/"> Daehyung Park</a> in the <a href="https://rirolab.github.io/">RIRO (Robust Intelligence & RObotics) Lab</a>, I am currently working on learning constraints—such as safety conditions and task specifications—from demonstrations. By incorporating these constraints into reinforcement learning, I aim to develop policies that are both generalizable and safe in real-world applications.</p>
                <p>I received my B.S. from KAIST in 2023, where I majored in Mechanical Engineering and minored in Mathematical Sciences. I had the honor of being awarded the KAIST Presidential Fellowship. </p>
                <p style="text-align:center">
                  <a href="mailto:cmw9903@kaist.ac.kr">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV_MinwooCho_202502.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://github.com/MinchoU">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/minu/minu_2402.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/minu/minu_2402.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research Interest</h2>
                <p>
                    My research focuses on applying RL to real-world robotics, where several fundamental challenges such as uncertainty or sample efficiency still hinder deployment on physical robots. To address these challenges, my current research interests include:
                </p>
                <ul>
                    <li>
                        <b>RL with constraints:</b> Unlike rewards, constraints can effectively restrict unintended or unsafe behavior, which is crucial for real-world deployment. However, these constraints are often difficult to explicitly define and ensuring the learning of zero-violation policies in complex domains remains challenging. How can we effectively learn and leverage such constraints?
                    </li>
                    <br>
                    <li>
                        <b>Offline-to-Online RL (O2O):</b> Since collecting real-world data is expensive, O2O RL methods offer a promising approach to improve sample efficiency and bridge the sim-to-real gap. Further reducing the number of online interactions and applying these methods in safety-critical settings is essential for real-world deployment.
                    </li>
                    <br>
                    <li>
                        <b>Learning from video:</b> Learning task knowledge from videos can eliminate the need for handcrafted rewards in RL. Just as humans learn by observing, I'm interested in enabling robots to acquire high-quality skills from a diverse range of videos without requiring task-specific designs.
                    </li>
                </ul>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <!-- <td style="padding:8px;width:30%;vertical-align:middle">
      <h2>Publications</h2>
    </td> -->
    <tr>
      <td colspan="2" style="padding:8px; vertical-align:middle">
        <h2>Publications</h2>
      </td>
    </tr>
    <tr>
      <td style="padding:8px;width:30%;vertical-align:middle">
        <div class="one">
          <div class="two">
            <!-- <video width=150% muted autoplay loop> -->
            <video style="width:140%; margin-top:10px" muted autoplay loop>
              <source src="images/minu/ilcl_short_real_reduced.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </td>
      <td style="padding:16px;width:70%;vertical-align:top">
        <!-- <p><b>ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations</b></p> -->
        <a href="data/768_ILCL_Inverse_Logic_Constra.pdf"><b>ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations</b></a>
        <!-- <a href="https://relight-to-reconstruct.github.io/">
          <span class="papertitle">Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation</span>
        </a> -->
        <br>
        <strong>Minwoo Cho</strong>,
        <a href="https://wognl0402.github.io/">Jaehwi Jang</a>,
        <a href="https://sites.google.com/site/daehyungpark//">Daehyung Park</a>
        <br>
        <em>Under review</em>, 2025
        <br>
        <!-- <a href="https://relight-to-reconstruct.github.io/">project page</a> /
        <a href="https://arxiv.org/abs/2412.15211">arXiv</a> -->
        <p></p>
        <p>
          In this work, we learn free-form temporal logic constraints from demonstrations using a tree-based genetic algorithm and constraint-regularized RL with logic cost redistribution.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:8px;width:30%;vertical-align:middle">
        <div class="one">
          <div class="two">
            <!-- <video width=150% muted autoplay loop> -->
            <video style="width:140%; margin-top:10px" muted autoplay loop>
              <source src="images/minu/rita_short_reduced.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </td>
      <td style="padding:16px;width:70%;vertical-align:top">
        <!-- <p><b>ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations</b></p> -->
        <a href="data/RiTA_2022__Dohyun_.pdf"><b>Natural Language-Guided Semantic Navigation using Scene Graph</b></a>
        <!-- <a href="https://relight-to-reconstruct.github.io/">
          <span class="papertitle">Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation</span>
        </a> -->
        <br>
        <a href="https://scholar.google.com/citations?user=Hbb9XlEAAAAJ">Dohyun Kim<sup>*</sup></a>,
        <a href="https://github.com/kjwoo31">Jinwoo Kim<sup>*</sup></a>,
        <strong>Minwoo Cho</strong>,
        
        <a href="https://sites.google.com/site/daehyungpark//">Daehyung Park</a>
        <br>
        <em>RiTA</em>, 2022. <b>Best student paper</b>
        <br>
        <!-- <a href="https://relight-to-reconstruct.github.io/">project page</a> /
        <a href="https://arxiv.org/abs/2412.15211">arXiv</a> -->
        <p></p>
        <p>
          In this work, we perform semantic navigation using a scene-graph grounding network that predicts the object from a natural language input, on RBQ-3 quadruped robot.
        </p>
      </td>
    </tr>

    <tr>
      <td colspan="2" style="padding:8px; vertical-align:middle">
        <h2>Projects</h2>
      </td>
    </tr>
    <tr>
      <td style="padding:8px;width:30%;vertical-align:middle">
        <div class="one">
          <div class="two">
            <!-- <video width=150% muted autoplay loop> -->
            <video style="width:140%; margin-top:10px" muted autoplay loop>
              <source src="images/minu/iitp_manip_short_reduced.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </td>
      <td style="padding:16px;width:70%;vertical-align:top">
        <!-- <p><b>ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations</b></p> -->
        <b>Manipulation of daily objects with visual reinforcement learning</b>
        <!-- <a href="https://relight-to-reconstruct.github.io/">
          <span class="papertitle">Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation</span>
        </a> -->
        <br>
        Project at <em><a href="https://rirolab.github.io/">RIRO Lab</a></em>, 2024
        <br>
        <!-- <a href="https://relight-to-reconstruct.github.io/">project page</a> /
        <a href="https://arxiv.org/abs/2412.15211">arXiv</a> -->
        <p></p>
        <p>
          This project aims to learn RL policies for manipulating everyday objects in diverse rigid and deformable scenarios—such as pick-and-place and entangling/disentangling—directly from visual input. We build on <a href="https://github.com/rail-berkeley/serl">SERL</a> as the code base.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:8px;width:30%;vertical-align:middle">
        <div class="one">
          <div class="two">
            <!-- <video width=150% muted autoplay loop> -->
            <video style="width:140%; margin-top:10px" muted autoplay loop>
              <source src="images/minu/metaicl.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </td>
      <td style="padding:16px;width:70%;vertical-align:top">
        <!-- <p><b>ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations</b></p> -->
        <a href="https://www.youtube.com/watch?v=O2O1TUxWMas"><b>Learning common constraints from multi-task demonstrations without rewards</b></a>
        <!-- <a href="https://relight-to-reconstruct.github.io/">
          <span class="papertitle">Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation</span>
        </a> -->
        <br>
        Course project in CS672 (Reinforcement Learning)</a></em>, 2023
        <br>
        <!-- <a href="https://relight-to-reconstruct.github.io/">project page</a> /
        <a href="https://arxiv.org/abs/2412.15211">arXiv</a> -->
        <p></p>
        <p>
          This project aims to infer common constraints from multi-task demonstrations by decomposing the task-agnostic part of the reward inferred from Meta-IRL.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:8px;width:30%;vertical-align:middle">
        <div class="one">
          <div class="two">
            <!-- <video width=150% muted autoplay loop> -->
            <video style="width:140%; margin-top:10px" muted autoplay loop>
              <source src="images/minu/cs470_images.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </td>
      <td style="padding:16px;width:70%;vertical-align:top">
        <!-- <p><b>ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations</b></p> -->
        <a href="data/CS477_poster.pdf"><b>Learning based navigation planning for legged robots in challenging terrain</b></a>
        <!-- <a href="https://relight-to-reconstruct.github.io/">
          <span class="papertitle">Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation</span>
        </a> -->
        <br>
        Course project in CS477 (Introduction to AI)</a></em>, 2022
        <br>
        <!-- <a href="https://relight-to-reconstruct.github.io/">project page</a> /
        <a href="https://arxiv.org/abs/2412.15211">arXiv</a> -->
        <p></p>
        <p>
          This project aims to generate a navigation plan based on the traversability prediction network. Traversabilities are estimated from the voxels of the terrain and trained with the locomotion performance of RL policies in various terrains.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:8px;width:30%;vertical-align:middle">
        <div class="one">
          <div class="two">
            <!-- <video width=150% muted autoplay loop> -->
            <video style="width:140%; margin-top:10px" muted autoplay loop>
              <source src="images/minu/hovercraft_reduced.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </td>
      <td style="padding:16px;width:70%;vertical-align:top">
        <!-- <p><b>ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations</b></p> -->
        <a href="https://www.youtube.com/watch?v=FIOqoE0QkxQ"><b>Self-driving hovercraft</b></a>
        <!-- <a href="https://relight-to-reconstruct.github.io/">
          <span class="papertitle">Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation</span>
        </a> -->
        <br>
        Course project in ME400 (Capstone Design)</a></em>, 2022
        <br>
        <!-- <a href="https://relight-to-reconstruct.github.io/">project page</a> /
        <a href="https://arxiv.org/abs/2412.15211">arXiv</a> -->
        <p></p>
        <p>
          This project aims to design and create a self-driving hovercraft to navigate to given goals using a 2D LiDAR and BLDC motors. We designed the hovercraft with two thrust propellers, and controlled the machine using PD control.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:8px;width:30%;vertical-align:middle">
        <div class="one">
          <div class="two">
            <!-- <video width=150% muted autoplay loop> -->
            <video style="width:140%; margin-top:10px" muted autoplay loop>
              <source src="images/minu/ME454_goldberg.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </td>
      <td style="padding:16px;width:70%;vertical-align:top">
        <!-- <p><b>ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations</b></p> -->
        <a href="https://youtu.be/vvtrHWR49tA"><b>Goldberg machine simulation in Gazebo</b></a>
        <!-- <a href="https://relight-to-reconstruct.github.io/">
          <span class="papertitle">Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation</span>
        </a> -->
        <br>
        Course project in ME454 (Dynamic System Programming)</a></em>, 2022
        <br>
        <!-- <a href="https://relight-to-reconstruct.github.io/">project page</a> /
        <a href="https://arxiv.org/abs/2412.15211">arXiv</a> -->
        <p></p>
        <p>
          This project aims to creatively design a Goldberg machine in a Gazebo simulator. Enjoy the video!
        </p>
      </td>
    </tr>

    <table>
      <tr>
        <td>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>. 
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
